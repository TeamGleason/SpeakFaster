## Python environment

It is highly recommended that you do Python development in a virtualenv.

In install the required dependencies in the virtualenv, do:

```sh
python install -r requirements.txt
```

## Postprocessing

### Audio Event Classification

We use [YAMNet](https://tfhub.dev/google/lite-model/yamnet/tflite/1)
to extract audio event labels from input audio files.

Command line example:

```sh
python extract_audio_events.py testdata/test_audio_1.wav /tmp/audio_events.tsv
```

### Visual Object Detection

We use [SSD on MobileNetV2](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1) to detect visual objects in images captures from camera(s).

Command line example for an input video file (e.g., an .mp4 file):

```sh
python detect_objects.py \
    --input_video_path testdata/test_video_1.mp4 \
    --output_tsv_path /tmp/visual_objects.tsv
```

Command line example for a series of image files specified by a glob pattern:

```sh
python detect_objects.py \
    --input_image_glob 'testdata/pic*-standard-size.jpg' \
    --frame_rate 2 \
    --output_tsv_path /tmp/visual_objects.tsv
```

Note the test images in the testdata/ folder are under the CC0 (public domain)
license and are obtained from the URLs such as:
- https://search.creativecommons.org/photos/10590078-2f13-4caf-b96d-5d1db14eccd4
- https://search.creativecommons.org/photos/832045ea-53f3-4a3d-9c35-9b51f9add43d

## Automatic Speech Recognitino (ASR, Speech-to-text) on audio files

The SpeakFaster Observer writes audio data to .flac files that are approximately
60 second each in length. To transcribe a consecutive series of such .flac files,
find the path to the first .flac file in the series and feed it to the audio_asr.py
script. Additionally, provide path to the  output .tsv file. For example:

```sh
python audio_asr.py data/20210710T095258428-MicWaveIn.flac /tmp/speech_transcript.tsv
```

The script automatically finds consecutive audio files in the same directory as the
first audio file based on the length of each audio file and the timestamps in the
file names.

To perform ASR and speaker diarization at the same time, use the `--speaker_count`
argument. For example:

```sh
python audio_asr.py \
    --speaker_count=2 \
    data/20210710T095258428-MicWaveIn.flac /tmp/speech_transcript.tsv
```

The speaker count must be known beforehand. In the .tsv file, the `Content`
column with contain the speaker index (e.g., "Speaker 2") appended to the
transcripts.

# TOOD(cais): Add instructions for Google Cloud credentials.

## Running unit tests in this folder

Use:

```sh
./run_tests.sh
```

## Processing a raw observer data session

### Installing Python dependencies ffmpeg

As a prerequisite, make sure you have installed all the required Python:

```sh
pip install -r requirements.txt
```

ffmpeg is required for video processing. To install ffmpeg on Linux, do

```sh
sudo apt-get install ffmpeg
```

To install ffmpeg on Windows, download an ffmpeg build for Windows from
https://github.com/BtbN/FFmpeg-Builds/releases, then unzip the binary
files to an appropriate folder. Add the folder, e.g.,
"C:\Program Files\ffmpeg-n4.4-80-gbf87bdd3f6-win64-lgpl\bin", to the
Path environment variable.

To install ffmpeg on Mac, you can use homebrew:

```sh
brew install ffmpeg
```

### Processing raw data for ELAN

The following command processes a raw data session folder with the keypresses
prorobuf file along with audio recordings. It extracts audio event labels and
ASR transcripts (with tentative speaker IDs). These audio-based labels are
merged with the keypress data as a single merged.tsv file, which can be loaded
into ELAN.

These other files are also generated by the script and can be loaded into ELAN:
- concatenated_audio.wav: The concatenated audio file.
- A screenshots.mp4 video file that is the result of stitching together the
  screenshot .jpg files based on their timestamps, or in case the screenshot
  .jpg files are unavailable:
  - A dummy screenshots video file

```sh
python elan_format_raw.py \
    /home/cais/sf_observer_data/session_3_with_screenshots/ \
    US/Eastern
```

In case screenshot image files are missing from an input directory, use the
`--dummy_vide_frame_image_path` flag to let the script generate a static
dummy video:

```sh
python elan_format_raw.py \
    /home/cais/sf_observer_data/session_2_with_keypresses/ \
    US/Eastern \
    --dummy_video_frame_image_path="${HOME}/SpeakFaster/Observer/SpeakFasterObserver Decoder/testdata/generic_windows_desktop.jpg"
```

### Postprocessing curation result

Based on the Data Curation Playbook, you should export a TSV file named
`curated.tsv` to the data-session directory at the end of the manual curation
process. Once this file has been exported, use the `elan_process_curated.tsv`
script to perform quality check and final conversion on  the file. Example
command line:

```sh
python \
    /home/cais/sf_observer_data/session_5_practice_conversation_2 \
    path/to/speaker_map.tsv
```

The first argument is the path to directory where the `curated.tsv` is located.
The second argument is the path to a TSV file that maps speaker's real first names
to their respective pseudonyms. It is assumed to have two columns: `RealName`
and `Pseudonym`. E.g.,

```tsv
RealName\tPseudonym
Sean\tUser001
Sherry\tParnter001
```

The `elan_process_curated.tsv` script may identify the following types of errors
(an incomplete list):

- Incorrect tier names
- tEnd value less than tBegin value
- A row of the `SpeechTranscript` tier contains no speaker tag such as
  `[Speaker:Sherry]` at the end.
- Incorrect keypress redaction time range format.

When you run into these errors, go back to ELAN, fix the problem and re-export
the `curated.tsv` file and re-run the `elan_process_curated.tsv`. Fix all problem
until the script says "Success..." and exports a file in the same directory named
`curated_processed.tsv`. This new TSV file is ready for data ingestion.
